(this["webpackJsonpcomputational-neurology-charite.github.io"]=this["webpackJsonpcomputational-neurology-charite.github.io"]||[]).push([[0],{25:function(e,t,a){e.exports=a(67)},33:function(e,t,a){},43:function(e,t,a){},44:function(e,t,a){},45:function(e,t,a){},46:function(e,t,a){},63:function(e,t,a){},64:function(e,t,a){},65:function(e,t,a){},66:function(e,t,a){},67:function(e,t,a){"use strict";a.r(t);var n=a(0),i=a.n(n),r=a(22),l=a.n(r),o=(a(33),a(3)),s=a(2),c=a(4);var m=function(){return i.a.createElement(i.a.Fragment,null,i.a.createElement("header",null,i.a.createElement("div",{className:"header-content"},i.a.createElement(o.b,{to:"/",style:{textDecoration:"none",color:"inherit"}},i.a.createElement("h1",{align:"center"},"Computational Neurology")))),i.a.createElement("nav",null,i.a.createElement(c.Link,{to:"home",smooth:!0,duration:500},"News"),i.a.createElement(c.Link,{to:"projects",smooth:!0,duration:500},"Projects"),i.a.createElement(c.Link,{to:"team",smooth:!0,duration:500},"Team"),i.a.createElement(c.Link,{to:"publications",smooth:!0,duration:500},"Publications"),i.a.createElement(c.Link,{to:"footer",smooth:!0,duration:500},"Contact"),i.a.createElement(o.b,{to:"/video_challenge"},"Video Challenge")))};a(43);const d=[{id:1,title:"Computational Inpatient Monitoring",shortDescription:"Predicting patient outcomes using routinely collected data",fullDescription:"The increasing availability of multimodal and continuous 'physiomes' of neurological patients through digital neuromonitoring, new sensor and wearable technologies paired with advanced data analytics offer opportunities for a fundamental transformation in neurology. For inpatient monitoring in intensive care, stroke medicine and epilepsy computational neurology provides vast opportunities for the development of predictive diagnostic methods for specific and time-critical therapies. By leveraging cutting-edge machine learning techniques, our team aims to transform the raw datasets into actionable intelligence, improving the accuracy, efficiency, and scalability of diagnostic processes. Our work focuses on automating the analysis of biosignals and other clinical data to enhance real-time decision-making, reduce the burden on healthcare professionals, and streamline hospital workflows. By seamlessly integrating AI models into clinical environments, we seek to drive innovation in patient care, ultimately improving outcomes and paving the way for a more data-driven healthcare future.",image:"/images/project_dwc.png"},{id:2,title:"Computational Ambulatory Monitoring",shortDescription:"Predicting patient outcomes using routinely collected data",fullDescription:"Enabled by new, wearable sensors and computational analysis, objective methods for continuous, longitudinal monitoring and risk assessments provide opportunities for a fundamental medical transformation towards personalized, proactive and time-critical therapies in the outpatient setting.",image:"/images/Fig_Wearable_DL.jpg"},{id:3,title:"Critical Dynamics in Brain Networks",shortDescription:"Criticality as the optimal set-point of network dynamics",fullDescription:"The brain's ability to process and integrate information across spatial and temporal domains is central to intact cognitive function. Physics and information theory have provided a framework describing an optimal state of information processing. This critical state, poised at the phase transition between chaotic and ceasing neuronal activity, is characterized by an equilibrium between excitation and inhibition in the neuronal network. When a network of neurons operates near a critical phase transition point, a range of information processing functions, including information transmission, integration, storage, dynamic range, and sensitivity to inputs, are optimized simultaneously. While criticality provides a framework connecting network structure and dynamics, whether it truly optimizes human cognition has yet to be shown. We investigate this using iEEG and MRI together with cognitive assessments.",image:"/images/criticality.jpg"},{id:4,title:"Neuro-AI",shortDescription:"Critical dynamics in artificial intelligence networks",fullDescription:"Deep Neural Networks (DNNs) have revolutionized numerous fields, yet their training and design remain challenging due to vast parameter spaces and limited theoretical understanding. We here aim to bridge the gap between neuroscience and artificial intelligence to unlock the full potential of these networks. Our research demonstrates how insight from neurology, biology and physics can innovate and enhance modern DNNs and training methods. Conversely, advancements in AI offer valuable perspectives that deepen our understanding of biological neural processes. We believe that combining ideas from neuroscience and AI is essential to exploring new frontiers in both disciplines.",image:"/images/critical_artificial_nn.png"},{id:5,title:"motus med",shortDescription:"Transforming the epilepsy diagnostic pathway through accessible & intelligent movement analysis",fullDescription:i.a.createElement("div",null,i.a.createElement("a",{href:"https://motusmed.de"},"motus med")," is a video analysis-based digital health tool intended to assist in the diagnosis and monitoring of persons with abnormal movements, seizures, or epilepsy. Videos of suspected videos are uploaded to our platform, undergo automated analysis to detect movement patterns characteristic of seizures, and can be securely shared with a specialist for additional visual review. Through motus med, we apply several vision based AI models that we have developed in the lab. motus med integrates with a variety of smartphones and home cameras in order to provide a device agnostic, flexible, and scalable digital solution. The analysis results are provided to users to assist in decision-making during the diagnostic stage and for ongoing monitoring and management of disease activity in persons with epilepsy."),image:"/images/motusmed.png"},{id:6,title:"ALVEEG",shortDescription:"Ambulatory long-term video-EEG monitoring",fullDescription:i.a.createElement("div",null,i.a.createElement("a",{href:"https://www.alveeg.de"},"ALVEEG")," is a prospective, multicentre, randomized and controlled intervention study. In Germany, people with seizure disorders often have to wait months for a long-term video-EEG which is traditionally only performed in specialized hospitals. This can greatly delay the correct diagnosis and treatment. New sensor technologies and data analysis supported by artificial intelligence are opening up new diagnostic approaches. The aim of the project is to improve the care of people with seizure disorders by providing access to long-term video-EEGs in the home setting."),image:"/images/project_alveeg.jpg"},{id:7,title:"Medical Edge AI",shortDescription:"M/EDGE",fullDescription:"With their close integration of programmable microelectronics, sensors and actuators, modern medical devices have opened up fundamentally new diagnostic and therapeutic possibilities. These devices require integration of artificial intelligence and autonomy directly in the medical device, i.e. medical edge computing. Together with partners from academia, med tech and semiconductor industries, the M/EDGE project aims to develop an electronics platform for highly integrated medical edge artificial intelligence.",image:"/images/medge-logo.png"}];function u(e){let{project:t,isExpanded:a,onClick:n}=e;return i.a.createElement("div",{className:"project-card "+(a?"expanded":""),onClick:n},a?null:i.a.createElement("img",{src:t.image,alt:t.title}),i.a.createElement("h3",null,t.title),i.a.createElement("p",null,a?t.fullDescription:t.shortDescription),a?null:i.a.createElement("body",{className:"more-text"},"More..."))}var p=function(){const[e,t]=Object(n.useState)(null);return i.a.createElement("section",{id:"projects"},i.a.createElement("h2",{className:"section-title"},"Projects"),i.a.createElement("div",{className:"projects-container"},d.map(a=>i.a.createElement(u,{key:a.id,project:a,isExpanded:e===a.id,onClick:()=>{return n=a.id,void t(e===n?null:n);var n}}))))};a(44);const h=[{name:"Prof. Dr. med. Dipl.-Phys. Christian Meisel",role:"Principal Investigator",image:"images/team/christian.jpg",desc:""},{name:"Alexander Nelde",role:"PhD Student",image:"/images/team/alex.jpg",desc:""},{name:"Dominik D. Kranz",role:"PhD Student",image:"/images/team/dominik.jpg",desc:"I studied Biophysics and love interdisciplinary research. My interests include pretty much everything that's cool, new and shiny, but my specialty is applying and adapting neural network architectures for biosignal processing, with a focus on ECG and EEG analysis. I especially enjoy bringing these models to the clinic, where they can help improve patient care. "},{name:"Ela Marie Akay",role:"Medical Doctor",image:"images/team/ela_picture.jpg",desc:"As a neurology resident, I am interested in neurovascular medicine and using Artificial Intelligence to improve patient outcomes in neurocritical care and stroke medicine. In my research, I use routinely collected data for neurocritical and stroke unit patients to generate insights into different neurovascular pathologies. I am also fascinated by the broader implications of AI applications in everyday clinical practice and effects on neurological patients and the healthcare system at large."},{name:"Laura Krumm",role:"PhD Student",image:"images/team/LauraK.png",desc:""},{name:"Dr. Markus Klammer",role:"Medical Doctor",image:"images/team/Markus.jpg",desc:"I am a clinical neurologist and specialize in the relationship between cardiac function and stroke. My interest lies in using deep learning on stroke imaging to explore patterns that link vascular physiology, brain injury, and patient outcomes, with the goal of improving prediction and personalized treatment strategies."},{name:"Dr. Maximilian Sch\xf6ls",role:"Medical Doctor",image:"images/team/Maximilian_Schoels.jpg",desc:""},{name:"Simon Giglhuber",role:"Master Student",image:"images/team/placeholder.jpg",desc:""},{name:"Jonas Stelzer",role:"Medical Student",image:"images/team/JonasS.jpg",desc:""},{name:"Mustafa Halimeh",role:"PhD Student",image:"/images/team/Mustafa.jpg",desc:"Mustafa is a computer scientist working on data-driven pipelines to allow better long-term monitoring and treatment of neurological disorders. His current research involves applying state-of-the deep learning models and analytic tools on data recorded from wearables and videos to detect and predict seizures in epilepsy."},{name:"Dr. Robert Terziev",role:"Medical Doctor",image:"/images/team/robert.jpg",desc:""},{name:"Simon Vock",role:"PhD Student",image:"images/team/SimonV.jpg",desc:"I am fascinated by the parallels between artificial neural networks and biological brains. My research focuses on critical phase transitions in machine learning, exploring how networks of simple units can give rise to complex, intelligent behavior. By applying insights from deep learning and physics, I study neural systems with the aim of advancing our understanding of both artificial and biological intelligence. Through this work, I hope to contribute to the development of more efficient AI systems and innovative treatments for neurological disorders."},{name:"Lennart J\xfcrgensen",role:"Medical Student",image:"images/team/placeholder.jpg",desc:""},{name:"Leon Neymeyer",role:"Working Student",image:"images/team/placeholder.jpg",desc:""},{name:"Tim Wiegand",role:"Medical Doctor",image:"images/team/tim.png",desc:"I am a neurology resident at Charit\xe9 and a postdoctoral researcher in the computational neurology group. My doctoral thesis focused on advanced neuroimaging techniques in neurotrauma and neurodegenerative disease. Currently, my research centers on predictive modeling in neurology. More specifically, I am working on forecasting increases in intracranial pressure based on time-series data from the ICU. I am co-author of \u201cK\xfcnstliche Intelligenz in der Medizin\u201d, a textbook on AI in medicine."},{name:"Claudia Gorski",role:"Administration",image:"images/team/claudia.jpg",desc:""}];function g(e){let{member:t,isExpanded:a,onClick:n,className:r=""}=e;return i.a.createElement("div",{className:`team-card ${a?"expanded":""} ${r}`,onClick:n},i.a.createElement("img",{src:t.image,alt:t.name}),i.a.createElement("h3",null,t.name),i.a.createElement("p",null,t.role),i.a.createElement("p",null,a?t.desc:""===t.desc?null:i.a.createElement("body",{className:"more-text"},"More...")))}var f=function(){const[e,t]=Object(n.useState)(null);return i.a.createElement("div",{className:"team-section-container"},i.a.createElement("section",{id:"team"},i.a.createElement("div",{className:"team-container-container"},i.a.createElement("h2",{className:"section-title team-title"},"Team"),i.a.createElement("div",{className:"team-container"},h.map(a=>i.a.createElement(g,{key:a.name,member:a,isExpanded:e===a.name,onClick:()=>{return n=a.name,void t(e===n?null:n);var n},className:"Prof. Dr. med. Dipl.-Phys. Christian Meisel"===a.name?"break-after":""}))))))};a(45);const E=["Dr. Amrit Kashyap (Postdoc)","Jessica Hochwald (Bachelor's Thesis)","Paul M\xfcller (Dr. rer. nat.)","Dr. Gadi Miron (Postdoc)","Lida Antonakopoulou (Internship)","Agustina Aragon Daud (Internship)","Lily Strittmatter (Bachelor's Thesis)","Mert Akg\xfcl (Bachelor's Thesis)","Mario Andina (Doctoral Thesis)","Georg von Arnim (Working Student)"];function y(){return i.a.createElement("section",{id:"alumni"},i.a.createElement("h2",{className:"section-title"},"Alumni"),i.a.createElement("div",{className:"alumni-card"},i.a.createElement("img",{src:"/images/logo_group.webp",alt:"Lab logo",className:"alumni-logo"}),i.a.createElement("ul",{className:"alumni-list"},E.map(e=>i.a.createElement("li",{key:e},e)))))}var v=a(23),b=a.n(v);a(46);function w(e){if(!e)return"";let t=e;const a={"&":"&","\\%":"%","\\#":"#","\\$":"$","\\_":"_","\\{":"{","\\}":"}","\\~{}":" ","\\^{}":"^","\\textbf{":"","\\textit{":"","\\texttt{":"","}":"","{":"","\\ ":" "};Object.keys(a).forEach(e=>{const n=new RegExp(e,"g");t=t.replace(n,a[e])});const n={"\\'{a}":"\xe1","\\`{a}":"\xe0","\\~{a}":"\xe3","\\^{a}":"\xe2","\\.{a}":"a","\\'e":"\xe9","\\`e":"\xe8","\\~e":"\u1ebd","\\^e":"\xea","\\.e":"e"};return Object.keys(n).forEach(e=>{const a=new RegExp(e,"g");t=t.replace(a,n[e])}),t=t.replace(/\\[a-zA-Z]+\{([^}]+)\}/g,"$1"),t}var k=function(){const[e,t]=Object(n.useState)([]),[a,r]=Object(n.useState)(null),[l,o]=Object(n.useState)("");Object(n.useEffect)(()=>{(async()=>{try{const e=await fetch("/publications.bib?ts="+Date.now()),a=await e.text(),n=b.a.toJSON(a).map(e=>({id:e.citationKey,title:w(e.entryTags.title),authors:e.entryTags.author,journal:w(e.entryTags.journal),book:w(e.entryTags.booktitle),year:e.entryTags.year,doi:e.entryTags.doi,url:e.entryTags.url,abstract:w(e.entryTags.abstract),entryType:e.entryType,publisher:e.entryTags.publisher,volume:e.entryTags.volume,pages:e.entryTags.pages}));t(n)}catch(e){console.error("Error fetching or parsing BibTeX file:",e)}})()},[]);const s=e.reduce((e,t)=>{const a=t.year||"Unknown Year";return e[a]||(e[a]=[]),e[a].push(t),e},{}),c=Object.keys(s).sort((e,t)=>t-e);return i.a.createElement("section",{id:"publications"},i.a.createElement("h2",{className:"section-title"},"Publications"),c.map(e=>i.a.createElement("div",{key:e,className:"year-section"},i.a.createElement("h3",null,e),i.a.createElement("div",{className:"year-container"},s[e].map(e=>i.a.createElement("div",{key:e.id,className:"publication-card "+(a===e.id?"expanded":""),onClick:()=>{return t=e.id,void r(e=>e===t?null:t);var t}},i.a.createElement("h4",null,e.title),i.a.createElement("p",{style:{color:"grey",marginBottom:"20px",marginTop:"20px"}},e.authors),i.a.createElement("p",null,i.a.createElement("i",null,e.journal||"Preprint")),i.a.createElement("body",{className:"more-text"},a===e.id?"":"More..."),a===e.id&&i.a.createElement(i.a.Fragment,null,e.abstract&&i.a.createElement("p",null,i.a.createElement("strong",null,"Abstract:")," ",e.abstract),e.doi&&i.a.createElement("p",null,i.a.createElement("strong",null,"DOI:")," ",i.a.createElement("a",{href:"https://doi.org/"+e.doi,target:"_blank",rel:"noopener noreferrer"},e.doi)),e.url&&i.a.createElement("p",null,i.a.createElement("strong",null,"URL:")," ",i.a.createElement("a",{href:e.url,target:"_blank",rel:"noopener noreferrer"},"View Publication")))))))))};var N=function(){return i.a.createElement("div",{id:"footer"},i.a.createElement("footer",null,i.a.createElement("p",null,"Contact:"," ",i.a.createElement("a",{href:"mailto:computational-neurology@charite.de"},"computational-neurology at charite.de")),i.a.createElement("p",null,"\xa9 ",(new Date).getFullYear()," Computational Neurology Research Group")))};a(24),a(61),a(62),a(63),a(16);a(64);function T(){return i.a.createElement("section",{id:"competition-announcement"},i.a.createElement("h2",{className:"section-title"},"Video-based Seizure Detection Challenge (2026)"),i.a.createElement("div",{className:"competition-content"},i.a.createElement("p",null,"In partnership with ",i.a.createElement("a",{href:"https://www.aiepilepsy-neuro.com/"},"The International Conference on Artificial Intelligence in Epilepsy and Other Neurological Disorders (2026)"),", the ",i.a.createElement("a",{href:"https://www.computational-neurology.org"},"Section on Computational Neurology at Charit\xe9 - Universit\xe4tsmedizin Berlin in Germany")," and partners are organizing a video-based seizure detection challenge."),i.a.createElement(o.b,{to:"/video_challenge",className:"competition-button"},"Details")))}a(65);function D(){return i.a.createElement("section",{id:"competition"},i.a.createElement("h2",{className:"section-title"},"Video-based Seizure Detection Challenge (2026)"),i.a.createElement("div",{className:"competition-intro"},i.a.createElement("p",null,"In partnership with ",i.a.createElement("a",{href:"https://www.aiepilepsy-neuro.com/"},"The International Conference on Artificial Intelligence in Epilepsy and Other Neurological Disorders (2026)"),", the ",i.a.createElement("a",{href:"https://www.computational-neurology.org"},"Section on Computational Neurology at Charit\xe9 \u2013 Universit\xe4tsmedizin Berlin")," and partners are organizing a video-based seizure detection challenge.")),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Key dates"),i.a.createElement("div",{className:"keydates"},i.a.createElement("div",{className:"k"},"Mid November 2025"),i.a.createElement("div",{className:"v"},"Challenge begins."),i.a.createElement("div",{className:"k"},"February 23, 2025"),i.a.createElement("div",{className:"v"},"Submission closes \u2014 submit early."),i.a.createElement("div",{className:"k"},"March 16\u201319, 2025"),i.a.createElement("div",{className:"v"},"Winners announced during the AI in Epilepsy 2026 conference."))),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Background"),i.a.createElement("p",null,i.a.createElement("strong",null,"Epilepsy")," affects about 50 million people worldwide. One severe early form, called ",i.a.createElement("strong",null,"infantile epileptic spasm syndrome (IESS)"),", occurs in roughly 1 in 2,000\u20132,500 infants during their first year of life. IESS causes sudden, repetitive movements of the head, arms, or legs known as ",i.a.createElement("strong",null,"epileptic spasms"),". These seizures are often mistaken for normal baby movements, leading to delays in diagnosis and treatment. Such delays can worsen long-term development and health outcomes. With the growing use of videos, smartphones and advances in ",i.a.createElement("strong",null,"artificial intelligence (AI)"),", new tools may help detect and diagnose these seizures more quickly [1].")),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Objective"),i.a.createElement("p",null,"The goal of this challenge is to develop an ",i.a.createElement("strong",null,"AI model")," that can automatically detect ",i.a.createElement("strong",null,"epileptic spasms")," from short video clips. Each clip is ",i.a.createElement("strong",null,"5 seconds long")," and may or may not contain a seizure event. To protect privacy, all videos have been ",i.a.createElement("strong",null,"anonymized using pose estimation"),", which tracks only key body landmarks rather than showing identifiable features. Each 5-second segment includes ",i.a.createElement("strong",null,"150 frames (30 frames per second)"),", and pose estimation has been applied to every frame, providing the coordinates of key points on the child\u2019s skeleton. Participants will receive a ",i.a.createElement("strong",null,"training dataset")," to build and train a ",i.a.createElement("strong",null,"binary classification model")," (seizure vs. no seizure). The submitted models will then be tested on a ",i.a.createElement("strong",null,"large unseen dataset")," to evaluate their accuracy and generalization.")),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Data description"),i.a.createElement("p",null,"The dataset consists of ",i.a.createElement("strong",null,"5-second video segments")," recorded using smartphones or other cameras from multiple children, some of whom were diagnosed with ",i.a.createElement("strong",null,"infantile spasms"),". Each segment may contain a ",i.a.createElement("strong",null,"seizure event")," (at any point within the 5 seconds) or show ",i.a.createElement("strong",null,"normal, seizure-free activity"),"."),i.a.createElement("p",null,"Within the training dataset, a single child may contribute ",i.a.createElement("strong",null,"multiple video segments"),", either from both classes (seizure and non-seizure) or from only one."),i.a.createElement("p",null,"To capture body movements while ensuring ",i.a.createElement("strong",null,"data privacy"),", all videos were processed using the ",i.a.createElement("strong",null,"MediaPipe")," library [2] to extract ",i.a.createElement("strong",null,"pose landmarks")," instead of sharing raw video frames. MediaPipe identifies ",i.a.createElement("strong",null,"33 body landmarks"),", each described by three spatial coordinates (x, y, z) and two confidence measures (",i.a.createElement("strong",null,"visibility")," and ",i.a.createElement("strong",null,"presence"),") that indicate how clearly each point is detected in the frame."),i.a.createElement("p",null,"Note that in some frames, landmarks may be missing due to ",i.a.createElement("strong",null,"video quality or motion artifacts"),". Pose extraction was performed using ",i.a.createElement("strong",null,"MediaPipe")," version 0.10.21 with the ",i.a.createElement("strong",null,"pose_landmarker_heavy.task")," model.")),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Training data"),i.a.createElement("p",null,i.a.createElement("strong",null,"The training data is available at the ",i.a.createElement("a",{href:"https://gitlab.com/computational-neurologie/video-based-seizure-detection-challenge#"},"repository"),". To be granted access, send an email to ",i.a.createElement("a",{href:"mailto:computational-neurology@charite.de"},"computational-neurology@charite.de")," including the following statement: \u201cI acknowledge that I will use the provided training data only for the official 2026 Video-based Seizure Detection Challenge, organized by the International Conference on Artificial Intelligence in Epilepsy and the Section on Computational Neurology at Charit\xe9 \u2013 Universit\xe4tsmedizin Berlin, and will not share or use it for any other purposes.\u201d")),i.a.createElement("p",null,"The training data consists of a set of .npy files containing the landmarks from each frame of the respective 5-second segments (in total 150 frames corresponding to 30 frames per second (fps)). Missed frames have \u2018nan\u2019 landmarks. Besides the .npy files, there is a .csv file (comma \u2018,\u2019 separated) containing the true labels of those video segments along with their names (two columns; first column: segment_name (string), second column: label (int 0 or 1; 1 for seizure, 0 no seizure)."),i.a.createElement("p",null,"The .npy files have the following naming convention:"),i.a.createElement("code",null,"child_","{","id","}","_","{","segment","}"),i.a.createElement("p",null,"where ",i.a.createElement("code",null,"id")," and ",i.a.createElement("code",null,"segment")," are both integers. Segments belonging to the same child have the same ",i.a.createElement("code",null,"id"),". Here is an example of code to read the landmarks .npy file:"),i.a.createElement("div",{className:"codeblock"},i.a.createElement("pre",null,i.a.createElement("code",null,"import numpy as np\n\n# Read a .npy file containing video 5-second segments landmarks\nlmk_arr = np.load('child_1_1.npy')\n# lmk_arr.shape -> (150, 33, 5) -> (n_frames, n_landmarks, [x, y, z, visibility, presence])\n# lmk_arr.dtype -> float64\n\n# Get frames with nan landmarks\nis_nan = np.isnan(lmk_arr)\nnan_inx = np.any(is_nan, axis=(1, 2))\n"))),i.a.createElement("p",null,"Here is an example of code to read the .csv file:"),i.a.createElement("div",{className:"codeblock"},i.a.createElement("pre",null,i.a.createElement("code",null,"import pandas as pd\n\ntrain_df = pd.read_csv('train_data.csv', sep=',')\n\n# Example:\n# segment_name,label\n# child_1_1.npy,1\n# child_1_2.npy,0")))),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Output data"),i.a.createElement("p",null,"Each submitted model must produce a single .csv file containing the predicted labels for all 5-second video segments. The file should have two columns:"),i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("strong",null,"file_name")," \u2013 the name of the corresponding ",i.a.createElement("code",null,".npy")," file"),i.a.createElement("li",null,i.a.createElement("strong",null,"label")," \u2013 the predicted class (",i.a.createElement("strong",null,"1")," = seizure, ",i.a.createElement("strong",null,"0")," = non-seizure)")),i.a.createElement("p",null,"Please note that predictions must be made for each 5-second segment individually, not for each child. The task is to detect whether a segment contains a seizure, not to classify whether a child has infantile spasms.")),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Performance metrics"),i.a.createElement("p",null,"Based on the predicted labels from each participant\u2019s model, we will compute the performance metrics of sensitivity, specificity, accuracy, and F1-score using an out-of-sample test dataset. These metrics will be computed as sample-based, i.e., each 5-second video segment will have a single prediction value of either 1 or 0. We will report the metrics as follows:"),i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("strong",null,"Sensitivity"),": TP / (TP + FN)"),i.a.createElement("li",null,i.a.createElement("strong",null,"Specificity"),": TN / (TN + FP)"),i.a.createElement("li",null,i.a.createElement("strong",null,"Precision"),": TP / (TP + FP)"),i.a.createElement("li",null,i.a.createElement("strong",null,"F1-score"),": 2 \xd7 (Precision \xd7 Sensitivity) / (Precision + Sensitivity)"))),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Evaluation"),i.a.createElement("p",null,"Submissions will be evaluated on an out-of-sample dataset using the above defined performance metrics. The F1 score will be used as a main metric to rank the performances and, in case of tie of F1 score values, sensitivity will be considered as a secondary ranking metric.")),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Awards"),i.a.createElement("p",null,"Our sponsors are generously offering $10,000 prizes for the winning participants. This prize money will be split among only the best two participant teams as follows:"),i.a.createElement("ul",{className:"awards"},i.a.createElement("li",null,i.a.createElement("strong",null,"First place:")," $7,000"),i.a.createElement("li",null,i.a.createElement("strong",null,"Second place:")," $3,000")),i.a.createElement("p",null,"Both winning participant teams (first/second place) are required to send their detailed solutions with the source code for final verification. This will demand them to agree on sharing their code under a CC-BY- SA 4.0 license to receive the full awarded prize money."),i.a.createElement("p",null,"The challenge organizers will write a summary of the outcome of the challenge in a leading journal in our field. The top challenge contributors will be invited to contribute to this paper to describe their methodology."),i.a.createElement("p",null,"To be eligible to receive the prize money, at least one co-author of the submitted model must be registered (in person or virtual) for the conference. Teams that are not registered can participate, but they will not be eligible to win.")),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Organizers"),i.a.createElement("p",null,"The challenge is organized by the ",i.a.createElement("a",{href:"https://www.computational-neurology.org"},"Section on Computational Neurology at Charit\xe9 - Universit\xe4tsmedizin Berlin in Germany"),", in collaboration with ",i.a.createElement("a",{href:"https://www.aiepilepsy-neuro.com/"},"The International Conference on Artificial Intelligence in Epilepsy and Other Neurological Disorders (2026)"),"."),i.a.createElement("p",null,"For questions to the organizers: ",i.a.createElement("a",{href:"mailto:computational-neurology@charite.de"},"computational-neurology@charite.de"))),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Legal statement"),i.a.createElement("p",null,"By downloading the data participants acknowledge that the shared data must only be used for this competition.")),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Submission guidelines"),i.a.createElement("p",null,"Participants should provide a pre-trained model packaged as a Docker image. The Docker image should be downloadable from an image registry. The training data contain an example of a Docker file. Participants can have a maximum of three different submissions. An abstract describing the methodology and performance of the training data is optional. Participants are invited to submit a poster in addition to the challenge submission, more details on poster submission are available on the ",i.a.createElement("a",{href:"https://www.aiepilepsy-neuro.com/"}," AI in epilepsy conference website"),"."),i.a.createElement("p",null,i.a.createElement("strong",null,"Docker requirements:")),i.a.createElement("p",null,"The docker image must contain two volumes, and define two environment variables:"),i.a.createElement("div",{className:"codeblock"},i.a.createElement("pre",null,i.a.createElement("code",null,'VOLUME ["/data"]\nVOLUME ["/output"]\nENV INPUT=""\nENV OUTPUT=""'))),i.a.createElement("p",null,"Where ",i.a.createElement("code",null,"INPUT")," and ",i.a.createElement("code",null,"OUTPUT")," variables contain the path to test-out dataset and .csv file relative to /data and /output.")),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Computing platform"),i.a.createElement("p",null,"All submitted models will be evaluated on our on-premises high-performance computing (HPC) system. During the evaluation process, the device will have no internet access to ensure data security and fairness across submissions. The evaluation will be performed on a DELL workstation equipped with:"),i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("strong",null,"2\xd7 NVIDIA RTX 6000 GPUs")," (48 GB RAM each)"),i.a.createElement("li",null,i.a.createElement("strong",null,"32 CPU cores")),i.a.createElement("li",null,i.a.createElement("strong",null,"Windows 11")," operating system with ",i.a.createElement("strong",null,"Ubuntu (WSL)")," support"))),i.a.createElement("div",{className:"competition-card"},i.a.createElement("h3",null,"Submission form"),i.a.createElement("p",{className:"smallprint"},"Please submit your algorithms via submission form (link below). The algorithms submitted here remain the strict property of the inventor. Neither the conference organizers nor the challenge sponsors have any intellectual property claims on the algorithms. The challenge organizers will not share or disseminate them after the evaluation of the model and only use them during the challenge to evaluate performance on a holdout dataset.")),i.a.createElement("a",{href:"https://forms.office.com/Pages/ResponsePage.aspx?id=ORnprz6SLEO8ZsvD7BjQLE1mFpKUXcBDnOuOUqQLSWVUREQ5WEJLVUdFQkE5VTVENVhCUjNJQ0NITCQlQCN0PWcu",target:"_blank",rel:"noopener noreferrer",className:"competition-button"},"Submit your entry"),i.a.createElement("div",{className:"refs"},i.a.createElement("h4",null,"References"),i.a.createElement("ol",null,i.a.createElement("li",null,"Miron, G., Halimeh, M., Tietze, S. et al. Detection of epileptic spasms using foundational AI and smartphone videos. npj Digit. Med. 8, 370 (2025). https://doi.org/10.1038/s41746-025-01773-1"),i.a.createElement("li",null,"Lugaresi, C., Tang, J., Nash, H., McClanahan, C., Uboweja, E., Hays, M., Zhang, F., Chang, C., Yong, M.G., Lee, J., Chang, W., Hua, W., Georg, M., & Grundmann, M. (2019). MediaPipe: A Framework for Building Perception Pipelines. https://doi.org/10.48550/arXiv.1906.08172"))))}var z=function(){return i.a.createElement(o.a,null,i.a.createElement("div",{className:"App"},i.a.createElement(m,null),i.a.createElement(s.c,null,i.a.createElement(s.a,{path:"/",element:i.a.createElement("main",null,i.a.createElement(T,null),i.a.createElement(p,null),i.a.createElement(f,null),i.a.createElement(y,null),i.a.createElement(k,null))}),i.a.createElement(s.a,{path:"/video_challenge",element:i.a.createElement(D,null)})),i.a.createElement(N,null)))};var C=e=>{e&&e instanceof Function&&a.e(3).then(a.bind(null,68)).then(t=>{let{getCLS:a,getFID:n,getFCP:i,getLCP:r,getTTFB:l}=t;a(e),n(e),i(e),r(e),l(e)})};a(66);l.a.createRoot(document.getElementById("root")).render(i.a.createElement(i.a.StrictMode,null,i.a.createElement(z,null))),C()}},[[25,1,2]]]);
//# sourceMappingURL=main.112695a1.chunk.js.map